# Bibliography and Citation Database

## Key Papers by Category

### Humanoid Robotics & Embodied AI

1. **Siciliano, B., et al. (2023).** "Service Robotics: Recent Advances and Future Directions." *IEEE Robotics & Automation Magazine*, 30(2), 12-25.

2. **Deitke, M., et al. (2022).** "Embodied AI: Bridging Simulation and Reality." *NeurIPS Datasets and Benchmarks*.

3. **Ahn, M., et al. (2022).** "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances." *Conference on Robot Learning (CoRL)*.
   - **Key Contribution**: SayCan - LLM grounding in robot affordances
   - **Relevance**: LLM integration for task planning

4. **Brohan, A., et al. (2023).** "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control." *arXiv:2307.15818*.
   - **Key Contribution**: End-to-end VLA model for robotics
   - **Relevance**: Vision-language-action architecture

5. **Driess, D., et al. (2023).** "PaLM-E: An Embodied Multimodal Language Model." *ICML*.
   - **Key Contribution**: 562B parameter embodied multimodal LLM
   - **Relevance**: Multimodal fusion for robotics

### Natural Language Processing

6. **Radford, A., et al. (2021).** "Learning Transferable Visual Models From Natural Language Supervision." *ICML*.
   - **Key Contribution**: CLIP - vision-language contrastive learning
   - **Relevance**: Foundation for multimodal understanding

7. **Lewis, P., et al. (2020).** "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." *NeurIPS*.
   - **Key Contribution**: RAG architecture
   - **Relevance**: Knowledge retrieval for dialogue

8. **Touvron, H., et al. (2023).** "Llama 2: Open Foundation and Fine-Tuned Chat Models." *arXiv:2307.09288*.
   - **Key Contribution**: Open-source LLM with commercial license
   - **Relevance**: Base LLM for edge deployment

9. **Shridhar, M., et al. (2022).** "CLIPort: What and Where Pathways for Robotic Manipulation." *CoRL*.
   - **Key Contribution**: Language-conditioned imitation learning
   - **Relevance**: Language grounding in manipulation

10. **Liang, J., et al. (2023).** "Code as Policies: Language Model Programs for Embodied Control." *ICRA*.
    - **Key Contribution**: LLMs generating executable code for robots
    - **Relevance**: Reasoning and planning architecture

### Computer Vision

11. **Kirillov, A., et al. (2023).** "Segment Anything." *ICCV*.
    - **Key Contribution**: SAM - foundation model for segmentation
    - **Relevance**: Zero-shot segmentation capability

12. **Caron, M., et al. (2023).** "DINOv2: Learning Robust Visual Features without Supervision." *arXiv:2304.07193*.
    - **Key Contribution**: Self-supervised vision foundation model
    - **Relevance**: Feature extraction for perception

13. **Yang, L., et al. (2024).** "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data." *CVPR*.
    - **Key Contribution**: Robust monocular depth estimation
    - **Relevance**: Depth perception without specialized hardware

14. **Campos, C., et al. (2021).** "ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial, and Multimap SLAM." *IEEE Trans. Robotics*, 37(6), 1874-1890.
    - **Key Contribution**: State-of-the-art visual SLAM
    - **Relevance**: Localization and mapping

15. **Wang, C.-Y., et al. (2023).** "YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors." *CVPR*.
    - **Key Contribution**: Real-time object detection
    - **Relevance**: Efficient edge detection

### Multimodal Learning

16. **Alayrac, J.-B., et al. (2022).** "Flamingo: A Visual Language Model for Few-Shot Learning." *NeurIPS*.
    - **Key Contribution**: Few-shot multimodal learning
    - **Relevance**: Efficient adaptation to new tasks

17. **Li, J., et al. (2023).** "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models." *ICML*.
    - **Key Contribution**: Efficient vision-language alignment
    - **Relevance**: Multimodal fusion architecture

18. **Liu, H., et al. (2023).** "Visual Instruction Tuning." *NeurIPS*.
    - **Key Contribution**: LLaVA - instruction-following VLM
    - **Relevance**: Vision-language dialogue

19. **Owens, A., et al. (2018).** "Audio-Visual Scene Analysis with Self-Supervised Multisensory Features." *ECCV*.
    - **Key Contribution**: Audio-visual learning
    - **Relevance**: Multimodal sensor fusion

### Edge Computing & Acceleration

20. **Jacob, B., et al. (2018).** "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference." *CVPR*.
    - **Key Contribution**: Quantization-aware training
    - **Relevance**: Model optimization for edge

21. **Hinton, G., et al. (2015).** "Distilling the Knowledge in a Neural Network." *NeurIPS Deep Learning Workshop*.
    - **Key Contribution**: Knowledge distillation
    - **Relevance**: Model compression

22. **Cai, H., et al. (2020).** "Once-for-All: Train One Network and Specialize it for Efficient Deployment." *ICLR*.
    - **Key Contribution**: Efficient NAS for edge devices
    - **Relevance**: Hardware-aware model design

23. **NVIDIA (2023).** "NVIDIA Jetson AGX Orin: Technical Reference Manual."
    - **Key Contribution**: Edge AI platform specifications
    - **Relevance**: Hardware platform

### MLOps & Production Systems

24. **Shankar, S., et al. (2022).** "Operationalizing Machine Learning: An Interview Study." *arXiv:2209.09125*.
    - **Key Contribution**: MLOps best practices
    - **Relevance**: Production deployment

25. **Paleyes, A., et al. (2022).** "Challenges in Deploying Machine Learning: A Survey of Case Studies." *ACM Computing Surveys*, 55(6), 1-29.
    - **Key Contribution**: ML deployment challenges
    - **Relevance**: Real-world deployment insights

26. **Rabanser, S., et al. (2019).** "Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift." *NeurIPS*.
    - **Key Contribution**: Data drift detection methods
    - **Relevance**: Model monitoring

### ROS & Robotics Middleware

27. **Macenski, S., et al. (2022).** "Robot Operating System 2: Design, Architecture, and Uses In The Wild." *Science Robotics*, 7(66).
    - **Key Contribution**: ROS2 architecture and capabilities
    - **Relevance**: Core middleware

28. **Murali, A., et al. (2019).** "PyRobot: An Open-source Robotics Framework for Research and Benchmarking." *arXiv:1906.08236*.
    - **Key Contribution**: Python robotics framework
    - **Relevance**: High-level robot control

### Safety & Robustness

29. **Amodei, D., et al. (2016).** "Concrete Problems in AI Safety." *arXiv:1606.06565*.
    - **Key Contribution**: AI safety problem taxonomy
    - **Relevance**: Safety architecture design

30. **Hendrycks, D., et al. (2021).** "Natural Adversarial Examples." *CVPR*.
    - **Key Contribution**: Real-world robustness challenges
    - **Relevance**: Safety validation

31. **Berkenkamp, F., et al. (2017).** "Safe Model-based Reinforcement Learning with Stability Guarantees." *NeurIPS*.
    - **Key Contribution**: Safety-constrained RL
    - **Relevance**: Safe learning and adaptation

### Simulation & Sim-to-Real

32. **Zhao, W., et al. (2020).** "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: A Survey." *IEEE SSCI*.
    - **Key Contribution**: Sim-to-real techniques survey
    - **Relevance**: Simulation-based training

33. **Tobin, J., et al. (2017).** "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World." *IROS*.
    - **Key Contribution**: Domain randomization
    - **Relevance**: Robust simulation training

34. **NVIDIA (2022).** "Isaac Sim: Robotics Simulation and Synthetic Data Generation Platform."
    - **Key Contribution**: Physics-based robot simulation
    - **Relevance**: Training environment

### Human-Robot Interaction

35. **Goodrich, M. A., & Schultz, A. C. (2007).** "Human-Robot Interaction: A Survey." *Foundations and Trends in Human-Computer Interaction*, 1(3), 203-275.
    - **Key Contribution**: HRI fundamentals
    - **Relevance**: Interaction design principles

36. **Broadbent, E., et al. (2020).** "Robots in Aged Care: A Review of Perceptions." *Journal of Medical Internet Research*, 22(4), e15987.
    - **Key Contribution**: Healthcare robotics acceptance
    - **Relevance**: Application domain

### Memory & Cognitive Architecture

37. **Schrittwieser, J., et al. (2020).** "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
    - **Key Contribution**: MuZero - planning with learned models
    - **Relevance**: Reasoning and planning architecture

38. **Wayne, G., et al. (2018).** "Unsupervised Predictive Memory in a Goal-Directed Agent." *arXiv:1803.10760*.
    - **Key Contribution**: Episodic memory for agents
    - **Relevance**: Memory system design

### Task Planning & Reasoning

39. **Silver, T., et al. (2023).** "Generalized Planning in PDDL Domains with Pretrained Large Language Models." *AAAI*.
    - **Key Contribution**: LLM-based task planning
    - **Relevance**: High-level planning

40. **Singh, I., et al. (2023).** "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models." *ICRA*.
    - **Key Contribution**: LLM program synthesis for robots
    - **Relevance**: Task decomposition

### Manipulation & Grasping

41. **Tremblay, J., et al. (2018).** "Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects." *CoRL*.
    - **Key Contribution**: DOPE - 6-DoF pose estimation
    - **Relevance**: Manipulation perception

42. **Fang, K., et al. (2020).** "GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping." *CVPR*.
    - **Key Contribution**: Large-scale grasp dataset
    - **Relevance**: Grasp learning

### Additional Key Papers

43. **Bommasani, R., et al. (2021).** "On the Opportunities and Risks of Foundation Models." *arXiv:2108.07258*.
    - **Key Contribution**: Foundation models survey
    - **Relevance**: Architectural philosophy

44. **Pinto, L., & Gupta, A. (2017).** "Learning to Push by Grasping: Using Multiple Tasks for Effective Learning." *ICRA*.
    - **Key Contribution**: Multi-task manipulation learning
    - **Relevance**: Learning strategy

45. **Tellex, S., et al. (2020).** "Robots That Use Language." *Annual Review of Control, Robotics, and Autonomous Systems*, 3, 25-55.
    - **Key Contribution**: Language grounding survey
    - **Relevance**: NLP for robotics

46. **Koenig, N., & Howard, A. (2004).** "Design and Use Paradigms for Gazebo, An Open-Source Multi-Robot Simulator." *IROS*.
    - **Key Contribution**: Gazebo simulator
    - **Relevance**: Simulation platform

47. **Durrant-Whyte, H., & Henderson, T. C. (2016).** "Multisensor Data Fusion." *Springer Handbook of Robotics*, 867-896.
    - **Key Contribution**: Sensor fusion fundamentals
    - **Relevance**: Perception architecture

48. **Stasse, O., et al. (2017).** "TALOS: A New Humanoid Research Platform Targeted for Industrial Applications." *IEEE-RAS Humanoids*.
    - **Key Contribution**: Modern humanoid platform
    - **Relevance**: Hardware reference

49. **Yamamoto, K., et al. (2019).** "Development of Human Support Robot as the research platform of a domestic mobile manipulator." *ROBOMECH Journal*, 6(1), 4.
    - **Key Contribution**: Toyota HSR platform
    - **Relevance**: Service robot design

50. **Spenko, M., et al. (2018).** "The DARPA Robotics Challenge Finals: Humanoid Robots To The Rescue." *Springer*.
    - **Key Contribution**: DRC lessons learned
    - **Relevance**: Humanoid capabilities and challenges

---

## Citation Management

### Tools
- **Zotero**: Reference management
- **BibTeX**: LaTeX integration
- **Google Scholar**: Citation tracking
- **Semantic Scholar**: Paper discovery

### Organization
- Create collections by topic
- Tag papers by relevance
- Track citation counts
- Note key contributions

---

## Additional Resources

### Recent Conferences (2023-2025)
- **ICRA 2024**: International Conference on Robotics and Automation
- **IROS 2024**: Intelligent Robots and Systems
- **CoRL 2024**: Conference on Robot Learning
- **RSS 2024**: Robotics: Science and Systems
- **NeurIPS 2024**: Neural Information Processing Systems
- **CVPR 2024**: Computer Vision and Pattern Recognition
- **ICCV 2025**: International Conference on Computer Vision

### Key Journals
- *IEEE Transactions on Robotics*
- *International Journal of Robotics Research*
- *Science Robotics*
- *Autonomous Robots*
- *Journal of Field Robotics*

### Preprint Servers
- arXiv.org (cs.RO, cs.CV, cs.AI, cs.LG)
- OpenReview.net
- Papers with Code

---

## Citation Style Guide

**IEEE Format** (Recommended for robotics):
```
[1] A. Author, B. Author, and C. Author, "Title of paper," in Proc. Conference Name, Year, pp. 1-10.
```

**APA Format** (Alternative):
```
Author, A., Author, B., & Author, C. (Year). Title of paper. Conference Name, pages.
```

---

## Notes
- Aim for 50+ high-quality citations
- Balance classic foundational papers with recent work (2020-2025)
- Include diverse perspectives and approaches
- Cite open-source tools and frameworks
- Give credit to industrial contributions (Tesla Bot, Figure AI, etc.)
- Include survey papers for broader context

