# ==============================================================================
# Humanoid Robot Assistant - Base System Configuration
# ==============================================================================

# System Identification
system:
  name: "humanoid_robot_assistant"
  version: "0.1.0"
  robot_id: "robot_001"  # Override per deployment
  deployment_env: "development"  # development, staging, production

# Cloud-Edge Architecture
architecture:
  mode: "hybrid"  # "edge_only", "cloud_only", "hybrid"
  edge:
    enabled: true
    device: "jetson_orin_agx"  # jetson_orin_agx, jetson_orin_nx, x86_gpu
    max_cpu_cores: 12
    max_memory_gb: 32
  cloud:
    enabled: true
    provider: "aws"  # aws, gcp, azure, on_prem
    region: "us-west-2"
    endpoint: "https://api.robot-brain-cloud.com"

# Service Registry (Microservices)
services:
  orchestrator:
    host: "localhost"
    port: 8000
    enabled: true
  
  nlp_service:
    host: "localhost"
    port: 8001
    enabled: true
    workers: 2
  
  vision_service:
    host: "localhost"
    port: 8002
    enabled: true
    workers: 2
  
  multimodal_service:
    host: "localhost"
    port: 8003
    enabled: true
    workers: 1
  
  planning_service:
    host: "localhost"
    port: 8004
    enabled: true
    workers: 1
  
  memory_service:
    host: "localhost"
    port: 8005
    enabled: true
    workers: 1
  
  perception_service:
    host: "localhost"
    port: 8006
    enabled: true
  
  safety_service:
    host: "localhost"
    port: 8007
    enabled: true
    priority: "critical"  # Always runs

# Natural Language Processing
nlp:
  intent_classifier:
    model_type: "transformer"  # transformer, hybrid, rules
    model_path: "models/nlp/intent_bert_v1.onnx"
    tokenizer: "bert-base-uncased"
    confidence_threshold: 0.7
    use_rules_fallback: true
    max_sequence_length: 128
  
  entity_extractor:
    # Tier 1: BERT-based NER (PRIMARY)
    tier1_model: "dslim/bert-base-NER"
    tier1_enabled: true
    # Tier 2: Custom fine-tuned model (FALLBACK)
    tier2_model_path: null  # Path to custom model when available
    tier2_enabled: false
    # Tier 3: spaCy (FALLBACK)
    tier3_model: "en_core_web_trf"  # or en_core_web_sm for CPU
    tier3_enabled: true
    confidence_threshold: 0.7
    use_gpu: null  # null = auto-detect
  
  dialogue:
    # Tier 1: Custom State Machine + Redis (PRIMARY)
    tier1_enabled: true
    redis_host: "localhost"
    redis_port: 6379
    redis_password: null
    session_ttl_minutes: 15
    # Tier 2: LangChain Memory (CONTEXT TRACKING)
    tier2_langchain_enabled: true
    context_window_turns: 10
    # Tier 3: In-memory fallback (AUTO)
    max_clarifications: 3
    enable_proactive_suggestions: true
  
  emotion:
    enabled: true
    # Tier 1: Emotion Transformer (PRIMARY)
    tier1_model: "j-hartmann/emotion-english-distilroberta-base"
    tier1_enabled: true
    # Tier 2: Sentiment Analysis (FALLBACK)
    tier2_model: "cardiffnlp/twitter-roberta-base-sentiment-latest"
    tier2_enabled: true
    # Tier 3: VADER Lexicon (FALLBACK)
    tier3_vader_enabled: true
    track_history: true
    history_length: 10
    confidence_threshold: 0.5
    use_gpu: null  # null = auto-detect
  
  rag:
    enabled: true
    # Vector Store Selection
    vector_store_type: "faiss"  # faiss or qdrant
    # Framework Selection
    use_langchain: true  # PRIMARY
    use_llamaindex: false  # FALLBACK
    # Embedding Model
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    use_gpu: null  # null = auto-detect
    # Vector Store Settings
    faiss:
      persist_dir: "./data/vector_store/faiss"
      index_type: "Flat"  # Flat, IVF, HNSW
    qdrant:
      host: "localhost"
      port: 6333
      collection_name: "robot_knowledge"
      use_memory: true  # In-memory for dev
    # Retrieval Settings
    chunk_size: 512
    chunk_overlap: 50
    top_k: 5
    score_threshold: 0.7
    top_k_retrieval: 5
    rerank: false
  
  llm:
    # Tier 1: OpenAI (CLOUD - PRIMARY)
    tier1_openai:
      enabled: true
      api_key: "${OPENAI_API_KEY}"  # From environment
      model: "gpt-4o-mini"  # gpt-4o-mini, gpt-4-turbo, gpt-4
      max_tokens: 256
      temperature: 0.7
      timeout: 30
    # Tier 2: Ollama (LOCAL - FALLBACK)
    tier2_ollama:
      enabled: true
      model: "llama3.2:3b"  # llama3.2:3b, phi3:mini, mistral:7b
      host: "http://localhost:11434"
      max_tokens: 256
      temperature: 0.7
    # Tier 3: Template-based (FALLBACK)
    tier3_template:
      enabled: true  # Always enabled
    # General Settings
    system_prompt: "You are a helpful humanoid robot assistant with vision and manipulation capabilities."
    use_rag_context: true
  
  asr:
    # Tier 1: Faster-Whisper (PRIMARY - Optimized Whisper.cpp)
    tier1_whisper:
      enabled: true
      model_size: "base"  # tiny, base, small, medium, large
      language: "en"
      use_gpu: null  # null = auto-detect
      compute_type: "int8"  # int8 (CPU), float16 (GPU)
      beam_size: 5
    # Tier 2: Vosk (FALLBACK - Lightweight, streaming)
    tier2_vosk:
      enabled: true
      model_path: null  # Download from alphacephei.com/vosk/models
      sample_rate: 16000
    # General Settings
    vad_enabled: true
    vad_threshold: 0.5
    streaming: false
  
  tts:
    # Tier 1: ElevenLabs (CLOUD - PRIMARY - Best Quality)
    tier1_elevenlabs:
      enabled: true
      api_key: "${ELEVENLABS_API_KEY}"  # From environment
      voice: "Adam"  # Adam, Bella, Antoni, etc.
      model: "eleven_monolingual_v1"
    # Tier 2: Coqui TTS (LOCAL - Good Quality)
    tier2_coqui:
      enabled: true
      model: "tts_models/en/ljspeech/vits"
      use_gpu: null  # null = auto-detect
    # Tier 3: pyttsx3 (LOCAL - Fast Fallback)
    tier3_pyttsx3:
      enabled: true
      rate: 175  # Words per minute
      voice: null  # null = default system voice
    # General Settings
    output_dir: "./audio_output"
    speaking_rate: 1.0

# Computer Vision
vision:
  input:
    image_size: [640, 640]  # Width, Height for model input
    fps: 30
    cameras:
      - id: "head_rgb"
        type: "rgb"
        resolution: [1920, 1080]
        device: "/dev/video0"
      - id: "head_depth"
        type: "depth"
        resolution: [640, 480]
        device: "realsense"
  
  detection:
    model: "yolov8n"  # yolov8n/s/m/l/x, yolo-nas
    model_path: "models/vision/yolov8n.engine"  # TensorRT engine
    confidence_threshold: 0.5
    nms_threshold: 0.45
    max_detections: 100
    classes: "coco"  # coco, custom
  
  segmentation:
    model: "sam"  # sam, mask2former
    model_path: "models/vision/sam_vit_b.onnx"
    mode: "automatic"  # automatic, prompt_based
  
  pose:
    human_pose:
      enabled: true
      model: "mediapipe"  # mediapipe, hrnet, openpose
      keypoints: 17
    object_pose:
      enabled: true
      model: "dope"  # dope, posecnn
  
  depth:
    source: "stereo"  # stereo, lidar, monocular
    monocular_model: "depth_anything"
    model_path: "models/vision/depth_anything_vitb.onnx"
  
  tracking:
    enabled: true
    tracker: "bytetrack"  # bytetrack, deepsort
    max_age: 30
    min_hits: 3

# Multimodal Fusion
multimodal:
  vision_language:
    model: "clip"  # clip, blip2, llava
    model_path: "models/multimodal/clip_vit_b32"
    embedding_dim: 512
  
  grounding:
    enabled: true
    confidence_threshold: 0.6
  
  vqa:
    enabled: true
    model: "blip2"
    model_path: "models/multimodal/blip2_opt_2.7b"

# Perception & SLAM
perception:
  slam:
    enabled: true
    backend: "orb_slam3"  # orb_slam3, rtabmap
    map_type: "3d"  # 2d, 3d
    relocalization: true
  
  localization:
    method: "visual_inertial"  # visual, visual_inertial, lidar
    publish_rate_hz: 10
  
  mapping:
    occupancy_grid:
      enabled: true
      resolution_m: 0.05
      size_m: [50, 50]
    point_cloud:
      enabled: true
      voxel_size_m: 0.01

# Task Planning
planning:
  task_planner:
    type: "hierarchical"  # hierarchical, pddl, learned
    planning_horizon_steps: 20
    replan_on_failure: true
  
  motion_planner:
    library: "moveit2"
    planning_time_s: 5.0
    max_velocity_scaling: 0.5
    max_acceleration_scaling: 0.5
  
  collision_avoidance:
    enabled: true
    safety_margin_m: 0.1
    prediction_horizon_s: 2.0

# Memory System
memory:
  episodic:
    enabled: true
    backend: "mongodb"
    max_episodes: 10000
    auto_summarize: true
  
  semantic:
    enabled: true
    backend: "postgresql"
    knowledge_graph: false
  
  working:
    enabled: true
    backend: "redis"
    ttl_seconds: 900  # 15 minutes
  
  vector_store:
    type: "faiss"  # faiss, milvus
    dimension: 384
    index_type: "IVF"  # Flat, IVF, HNSW
    nlist: 100

# Safety & Monitoring
safety:
  enabled: true
  watchdog:
    enabled: true
    check_interval_s: 1.0
    restart_on_failure: true
  
  anomaly_detection:
    enabled: true
    sensitivity: "medium"  # low, medium, high
  
  constraints:
    max_velocity_ms: 1.0
    max_force_n: 50.0
    workspace_limits_m:
      x: [-2.0, 2.0]
      y: [-2.0, 2.0]
      z: [0.0, 2.0]
  
  human_detection:
    enabled: true
    safety_distance_m: 0.5
    reduce_speed_near_humans: true
  
  emergency_stop:
    hardware_estop: true
    software_estop: true
    latency_ms_max: 50

# Logging & Monitoring
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "json"  # text, json
  outputs:
    - type: "console"
      enabled: true
    - type: "file"
      enabled: true
      path: "/var/log/robot/system.log"
      rotation: "1 day"
      retention: "30 days"
    - type: "sentry"
      enabled: false
      dsn: ""
  
  metrics:
    enabled: true
    prometheus:
      enabled: true
      port: 9090
    custom_metrics:
      - "inference_latency_ms"
      - "task_success_rate"
      - "safety_violations"

# Databases
databases:
  mongodb:
    uri: "${MONGODB_URI}"
    database: "robot_brain"
    collections:
      - "sessions"
      - "episodes"
      - "logs"
  
  postgresql:
    host: "${POSTGRES_HOST:localhost}"
    port: 5432
    database: "robot_semantic"
    user: "${POSTGRES_USER}"
    password: "${POSTGRES_PASSWORD}"
  
  redis:
    host: "${REDIS_HOST:localhost}"
    port: 6379
    db: 0
    password: "${REDIS_PASSWORD:}"

# External APIs
external_apis:
  openai:
    api_key: "${OPENAI_API_KEY}"
    organization: "${OPENAI_ORG:}"
  
  weather:
    provider: "openweathermap"
    api_key: "${WEATHER_API_KEY}"
  
  news:
    provider: "newsapi"
    api_key: "${NEWS_API_KEY}"

# Hardware Interfaces
hardware:
  ros2:
    domain_id: 0
    middleware: "cyclonedds"  # cyclonedds, fastdds
  
  cameras:
    backend: "ros2"  # ros2, opencv
    calibration_path: "config/calibration/"
  
  motors:
    interface: "ros2_control"
    controller_type: "joint_trajectory_controller"
  
  sensors:
    imu:
      enabled: true
      topic: "/imu/data"
      rate_hz: 100
    force_torque:
      enabled: true
      topic: "/ft_sensor/data"
      rate_hz: 100

# Performance Tuning
performance:
  gpu:
    device_id: 0
    memory_fraction: 0.8
    allow_growth: true
  
  threading:
    num_workers: 4
    batch_processing: true
  
  caching:
    enabled: true
    max_cache_size_gb: 2.0

# Feature Flags
features:
  multimodal_fusion: true
  emotion_detection: true
  proactive_assistance: false
  voice_cloning: false
  lifelong_learning: false
  fleet_knowledge_sharing: false

# Development
development:
  debug_mode: false
  simulation_mode: false
  mock_hardware: false
  verbose_logging: false
  save_debug_images: false

# ==============================================================================
# Environment Variables Required:
# - MONGODB_URI
# - POSTGRES_HOST, POSTGRES_USER, POSTGRES_PASSWORD
# - REDIS_HOST, REDIS_PASSWORD (optional)
# - OPENAI_API_KEY
# - WEATHER_API_KEY, NEWS_API_KEY
# ==============================================================================

